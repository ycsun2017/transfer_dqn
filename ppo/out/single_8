episode: 0, total reward: -174.786
episode: 10, total reward: -390.879
episode: 20, total reward: -120.92
episode: 30, total reward: -551.082
episode: 40, total reward: -78.54
episode: 50, total reward: -228.13
episode: 60, total reward: -289.076
episode: 70, total reward: -132.195
episode: 80, total reward: -113.606
episode: 90, total reward: -88.971
episode: 100, total reward: -301.625
episode: 110, total reward: -101.439
episode: 120, total reward: -268.58
episode: 130, total reward: -165.995
episode: 140, total reward: -145.255
episode: 150, total reward: -142.688
episode: 160, total reward: -114.547
episode: 170, total reward: -264.044
episode: 180, total reward: -236.876
episode: 190, total reward: -143.299
episode: 200, total reward: -96.924
episode: 210, total reward: -208.665
episode: 220, total reward: -34.262
episode: 230, total reward: -99.088
episode: 240, total reward: -113.453
episode: 250, total reward: -90.206
episode: 260, total reward: -230.171
episode: 270, total reward: -119.055
episode: 280, total reward: -248.49
episode: 290, total reward: -183.079
episode: 300, total reward: -225.041
episode: 310, total reward: -225.412
episode: 320, total reward: -197.424
episode: 330, total reward: -149.45
episode: 340, total reward: -175.057
episode: 350, total reward: -104.811
episode: 360, total reward: -427.388
episode: 370, total reward: -385.37
episode: 380, total reward: -276.002
episode: 390, total reward: -244.029
episode: 400, total reward: -136.29
episode: 410, total reward: -79.281
episode: 420, total reward: -44.229
episode: 430, total reward: -344.526
episode: 440, total reward: -109.168
episode: 450, total reward: -83.534
episode: 460, total reward: -267.583
episode: 470, total reward: -306.385
episode: 480, total reward: -260.999
episode: 490, total reward: -172.538
episode: 500, total reward: -166.896
episode: 510, total reward: -272.095
episode: 520, total reward: -91.216
episode: 530, total reward: -63.63
episode: 540, total reward: -163.427
episode: 550, total reward: -19.841
episode: 560, total reward: -434.233
episode: 570, total reward: -87.27
episode: 580, total reward: -248.242
episode: 590, total reward: -88.127
episode: 600, total reward: -91.072
episode: 610, total reward: -288.64
episode: 620, total reward: -119.918
episode: 630, total reward: -169.9
episode: 640, total reward: -111.255
episode: 650, total reward: -143.718
episode: 660, total reward: -224.805
episode: 670, total reward: -108.235
episode: 680, total reward: -95.621
episode: 690, total reward: -89.248
episode: 700, total reward: -145.627
episode: 710, total reward: -121.655
episode: 720, total reward: -209.085
episode: 730, total reward: -30.217
episode: 740, total reward: -113.799
episode: 750, total reward: -198.698
episode: 760, total reward: -111.615
episode: 770, total reward: -121.453
episode: 780, total reward: -70.579
episode: 790, total reward: -92.064
episode: 800, total reward: -105.204
episode: 810, total reward: -89.467
episode: 820, total reward: -52.517
episode: 830, total reward: -84.548
episode: 840, total reward: -56.86
episode: 850, total reward: -119.742
episode: 860, total reward: -191.792
episode: 870, total reward: -86.076
episode: 880, total reward: -142.406
episode: 890, total reward: -94.549
episode: 900, total reward: -48.396
episode: 910, total reward: -89.9
episode: 920, total reward: -81.262
episode: 930, total reward: -215.552
episode: 940, total reward: -93.606
episode: 950, total reward: -37.204
episode: 960, total reward: 36.999
episode: 970, total reward: -60.264
episode: 980, total reward: -51.658
episode: 990, total reward: -114.774
episode: 1000, total reward: -94.315
episode: 1010, total reward: -78.766
episode: 1020, total reward: -29.407
episode: 1030, total reward: -140.077
episode: 1040, total reward: 2.806
episode: 1050, total reward: -31.158
episode: 1060, total reward: -31.406
episode: 1070, total reward: -46.896
episode: 1090, total reward: -66.943
episode: 1100, total reward: -39.441
episode: 1110, total reward: -46.37
episode: 1120, total reward: -16.256
episode: 1130, total reward: -31.96
episode: 1140, total reward: -89.821
episode: 1150, total reward: -39.602
episode: 1160, total reward: -84.847
episode: 1170, total reward: 19.473
episode: 1180, total reward: -48.749
episode: 1190, total reward: -17.368
episode: 1200, total reward: -42.656
episode: 1210, total reward: -74.959
episode: 1230, total reward: -67.028
episode: 1250, total reward: -108.136
episode: 1280, total reward: -128.807
episode: 1290, total reward: -54.694
episode: 1350, total reward: -35.486
episode: 1360, total reward: -43.686
episode: 1390, total reward: -65.176
episode: 1400, total reward: 10.796
episode: 1420, total reward: 12.719
episode: 1430, total reward: -72.106
episode: 1450, total reward: 30.244
episode: 1470, total reward: 26.555
episode: 1490, total reward: -0.79
episode: 1540, total reward: 57.575
episode: 1690, total reward: 32.104
episode: 1770, total reward: 4.264
episode: 1780, total reward: -1.28
episode: 1800, total reward: -2.321
episode: 1850, total reward: 42.035
episode: 1870, total reward: 28.536
episode: 2050, total reward: -26.119
episode: 2080, total reward: -7.71
episode: 2110, total reward: -2.729
episode: 2530, total reward: 50.239
episode: 2550, total reward: 32.689
episode: 2560, total reward: 13.781
episode: 2580, total reward: -50.038
episode: 2640, total reward: 34.681
episode: 2910, total reward: -35.445
episode: 2940, total reward: 23.66
episode: 2950, total reward: -61.093
predict_state.0.weight Parameter containing:
tensor([[-3.1891e-01, -1.3762e-01, -6.3092e-02, -2.4812e-01, -6.8893e-02,
         -2.1239e-01,  2.0935e-01,  2.1234e-01, -2.4388e-01, -5.2438e-02,
         -7.6300e-02, -1.3156e-01],
        [ 2.7704e-01,  8.0510e-02,  3.7589e-02,  1.3842e-01, -2.9575e-01,
         -5.2028e-02, -3.2737e-02, -3.3904e-01,  1.6780e-01,  3.1409e-01,
          2.5312e-01,  2.8010e-01],
        [ 1.4059e-01,  2.3405e-01, -1.2794e-01,  2.6569e-01,  2.1211e-01,
         -2.9326e-01, -7.2596e-02, -6.3679e-02,  1.5563e-01, -5.9931e-03,
          1.8365e-01, -2.1476e-01],
        [-4.3943e-01,  3.7375e-01, -3.2369e-01, -1.8920e-01, -3.5425e-01,
         -1.9392e-01, -2.3950e-01, -2.3745e-01,  2.0371e-02,  1.8208e-02,
         -5.9625e-03,  1.7492e-02],
        [-4.9566e-02, -1.0018e-01,  2.4711e-01, -1.7955e-01, -1.6458e-03,
          2.0939e-01,  2.3564e-01, -2.4511e-01, -2.5147e-01, -1.6286e-01,
         -1.6472e-01, -2.5665e-01],
        [ 8.9048e-02, -6.7230e-02, -3.4877e-01, -3.0829e-01, -1.4173e-01,
          3.5705e-01, -4.9760e-02,  1.7535e-02,  1.5541e-01, -2.3399e-01,
          1.0553e-01, -3.4728e-02],
        [ 8.1361e-03, -1.5656e-01,  3.0306e-01, -1.4568e-01, -2.2784e-03,
          1.5239e-04, -1.5063e-01,  1.2989e-01,  2.8022e-01, -4.2638e-02,
          1.4945e-01,  2.0184e-01],
        [-1.8254e-01, -1.3468e-01,  2.7522e-02, -1.1233e-01,  2.3931e-01,
         -9.4845e-04, -2.2629e-01, -1.0228e-01,  2.2480e-01, -7.8047e-02,
          2.7539e-01, -1.2778e-01],
        [-1.9205e-02, -2.6886e-01,  2.0257e-02, -1.3937e-01, -2.4419e-01,
          1.6760e-01, -1.3238e-01, -2.8673e-01,  2.1754e-01,  1.3096e-01,
         -9.3955e-02,  1.3671e-01],
        [-2.6782e-01, -1.7452e-01, -7.1284e-02,  1.4275e-01,  2.1310e-01,
          2.4241e-01,  1.5018e-01, -3.0090e-01, -1.8699e-01, -8.4757e-02,
          1.8724e-02,  1.2502e-01],
        [-1.7302e-01, -1.0474e-01, -6.4633e-03, -2.7293e-01, -1.3286e-01,
         -2.9853e-01,  1.5333e-01,  1.2505e-01,  2.3976e-03, -2.2497e-01,
         -1.8431e-02, -2.4353e-01],
        [ 5.6122e-02,  1.8396e-01,  1.5222e-01,  3.3830e-02, -1.7347e-01,
          1.8167e-01,  1.8572e-01,  3.6667e-01,  2.7757e-01, -1.5063e-01,
          5.6076e-02,  6.1458e-02],
        [-2.5842e-01, -1.7078e-01,  2.0814e-01, -3.0580e-01,  3.0653e-01,
          1.3267e-01,  2.7013e-03, -2.5629e-01,  2.0804e-01, -1.8210e-03,
          9.6664e-02, -1.0476e-01],
        [ 5.1051e-02, -2.0314e-01, -3.8496e-01, -3.1586e-01,  1.0770e-02,
         -9.3714e-02, -2.5799e-01,  1.6104e-01, -1.4239e-01,  1.6156e-01,
          1.5316e-01, -1.3728e-01],
        [ 1.0098e-01, -2.7130e-01,  2.5079e-01, -2.0028e-01, -7.0578e-02,
         -1.5386e-01,  4.6473e-02, -5.2293e-02,  1.7464e-01,  6.5957e-02,
          2.6206e-01,  2.8770e-02],
        [-8.1541e-02, -1.0739e-01, -1.1645e-01,  2.5288e-01,  4.5421e-02,
         -4.6288e-03,  1.8707e-02,  2.3349e-01,  8.5428e-02,  1.3972e-01,
         -2.9784e-01, -1.6598e-01],
        [-2.1261e-01, -1.2485e-01,  8.5062e-02, -1.4969e-01, -1.1740e-01,
         -1.8318e-01,  1.4427e-01, -2.0937e-01, -6.0356e-02,  1.4969e-01,
          8.9467e-02,  1.2108e-01],
        [-1.3767e-01, -1.9340e-01,  2.5999e-01, -8.8767e-02, -4.0294e-02,
          9.5880e-03,  1.1293e-01,  1.6184e-01, -2.4881e-02, -2.4384e-01,
         -3.7906e-01, -5.8582e-02],
        [-2.3627e-01, -2.6607e-02, -3.2009e-01,  3.6351e-01,  2.2950e-02,
          3.9751e-01,  3.8119e-02,  2.2993e-01, -2.8290e-01, -1.9491e-01,
         -2.3909e-01, -6.3147e-02],
        [-1.1953e-01,  2.0404e-01, -2.6971e-01, -2.9054e-01,  3.5163e-01,
          1.3731e-01,  3.1688e-03, -1.8876e-01, -3.0534e-02,  2.2414e-01,
         -7.5945e-02,  7.2744e-02],
        [ 1.5361e-01, -3.5459e-02,  3.6046e-02,  2.6299e-01, -2.7314e-01,
          2.1280e-01, -1.9774e-01, -2.7942e-01, -2.6158e-01,  1.1983e-01,
         -1.4127e-01, -1.7487e-01],
        [-1.1213e-01,  6.1318e-02, -3.0992e-01, -1.2039e-02,  2.2903e-01,
          1.1931e-01, -7.5564e-02,  2.8520e-01,  1.9183e-01,  1.5490e-01,
          2.1580e-01,  7.2683e-02],
        [ 2.3190e-01,  5.9039e-01, -3.1467e-01, -2.4389e-01, -1.5092e-01,
         -4.6059e-01,  3.6520e-01, -4.2901e-01,  1.7987e-01,  1.9889e-01,
          1.2980e-01,  6.3349e-02],
        [-6.5628e-02, -5.2579e-02, -2.0074e-01, -1.9223e-01,  2.2466e-01,
          1.0149e-01,  4.7186e-02,  2.6779e-01, -2.0510e-01,  1.4544e-01,
          2.1584e-01, -1.4475e-01],
        [-1.6910e-01,  5.8987e-02,  1.2465e-03,  2.1606e-01,  1.1378e-01,
         -2.2176e-01,  4.6656e-02, -2.4127e-01,  1.2826e-01, -1.5040e-01,
          9.4605e-03, -6.2706e-02],
        [ 1.9913e-01, -2.3894e-02,  1.3993e-01, -2.7167e-01, -7.8057e-02,
          9.7349e-02,  3.7901e-02, -3.8081e-02,  1.8037e-01,  1.1255e-01,
         -2.0617e-01,  1.7472e-01],
        [-3.5673e-01, -4.2669e-02, -1.9731e-01, -2.2838e-02,  3.3205e-01,
          3.4198e-01,  2.1202e-01, -1.7019e-01, -2.3153e-01, -1.7491e-01,
         -1.9166e-01,  2.0404e-01],
        [ 3.5938e-02,  1.5974e-01, -6.3146e-02,  1.7491e-01,  1.9119e-01,
         -2.9542e-01, -1.2140e-01,  2.2540e-01,  1.0367e-01, -2.1519e-01,
         -3.0547e-01,  2.6753e-01],
        [ 2.0711e-01, -9.8819e-02, -1.9793e-01, -1.0270e-01,  1.5769e-01,
         -1.1668e-01,  6.0489e-02,  8.2794e-02, -1.9274e-01,  2.4801e-01,
          1.4450e-01,  1.9005e-01],
        [ 1.7334e-01, -1.3727e-01,  2.3588e-01, -1.2704e-01,  7.2929e-02,
          2.1457e-01, -9.2806e-02, -2.4213e-01, -1.0413e-01, -1.1517e-01,
          1.1634e-01,  2.9598e-02],
        [-2.1581e-01, -1.9141e-01, -4.5106e-01,  2.1575e-01, -2.3872e-01,
         -1.1880e-01, -1.0460e-01,  2.4583e-01,  2.2288e-01,  1.4393e-01,
          9.2791e-02, -6.3322e-02],
        [-1.4004e-01,  4.7349e-02,  8.4462e-02,  2.8309e-01, -2.4040e-02,
         -3.2898e-01, -1.2863e-01,  1.0456e-01,  1.0710e-01,  2.6053e-01,
          6.5890e-02,  1.9295e-01],
        [-2.4403e-01, -8.7847e-02,  1.5520e-01, -5.1132e-02,  1.3843e-01,
          3.0639e-01, -2.3663e-01, -3.1880e-01,  1.2081e-01, -1.4916e-01,
          2.3366e-02, -1.4196e-01],
        [-3.7514e-02, -5.1417e-03, -1.5156e-01, -7.9684e-03,  3.3565e-02,
          8.3974e-02, -6.8817e-02, -3.0075e-01, -9.3597e-02,  2.0136e-01,
         -9.3347e-02, -2.0067e-01],
        [-1.3175e-01, -2.3337e-01,  3.1716e-01, -7.3969e-02,  3.5524e-02,
          3.3513e-01, -1.4371e-01, -1.5610e-02,  6.4880e-02,  2.6736e-01,
         -9.3261e-02, -3.8725e-02],
        [-9.4042e-02, -7.7120e-02,  3.0034e-01, -1.7460e-01, -2.4060e-01,
          1.5130e-01,  1.3802e-01,  2.7417e-01, -1.7300e-01,  1.4292e-01,
         -8.2052e-02, -3.6391e-03],
        [-1.2578e-01,  3.2632e-02, -4.7517e-01, -4.6168e-01, -2.1576e-01,
          2.8469e-01, -4.5990e-02, -3.6482e-02, -1.5660e-01,  1.0456e-01,
         -3.0768e-02,  1.9060e-01],
        [ 4.1055e-02,  1.9244e-01,  6.5910e-02, -6.5779e-02,  1.9905e-01,
          2.0150e-01, -9.1456e-02,  4.7799e-03, -6.5175e-02, -1.7643e-01,
         -2.6401e-01, -1.5952e-01],
        [-2.6436e-01,  2.5500e-01, -1.1046e-01,  1.8500e-01, -1.8697e-01,
          2.0121e-02,  1.9931e-01, -1.9101e-02,  3.5638e-02,  1.8535e-01,
         -1.9761e-01, -9.9049e-02],
        [-4.3510e-01, -2.7166e-01,  1.6173e-02,  2.1911e-01, -1.0401e-01,
         -3.1661e-01,  2.4329e-02,  1.1541e-02,  2.1959e-01,  2.7220e-01,
          6.4718e-02,  2.4429e-01],
        [-2.8262e-01,  2.7273e-01,  1.5543e-01, -1.1810e-01, -6.5844e-02,
         -9.5750e-02,  1.9338e-01,  1.4193e-02, -2.1787e-01, -1.5992e-01,
         -2.4053e-01,  1.2194e-01],
        [ 2.2748e-01, -5.6141e-02, -3.5891e-01, -3.9124e-01,  8.0540e-02,
          9.8971e-02,  5.3665e-02,  3.0270e-01,  3.2537e-01, -5.0127e-02,
          2.4471e-01,  3.2408e-01],
        [-1.2634e-01, -2.8909e-01,  2.1301e-01,  1.5250e-01, -2.0837e-01,
         -4.2571e-02,  2.5623e-01, -2.7064e-02, -7.2440e-03, -8.3975e-02,
          5.9011e-03,  1.4185e-01],
        [-9.4064e-02, -1.7295e-01,  1.7745e-01, -2.8490e-01,  8.5555e-02,
          8.8953e-02,  1.0606e-01,  3.8883e-02, -9.8013e-03,  2.3829e-01,
         -1.0939e-01, -1.0020e-01],
        [-5.7170e-02,  1.4821e-01,  2.5642e-01,  1.4200e-01,  1.4214e-01,
          1.8503e-01, -1.2809e-01, -3.8878e-01, -1.7972e-01, -2.0909e-01,
          6.4637e-02,  2.1239e-01],
        [ 6.2261e-03, -1.8256e-01,  2.3600e-01, -1.5543e-02, -1.0048e-01,
          1.7132e-01,  2.0051e-01,  2.0116e-02, -2.2923e-01, -1.3599e-01,
         -8.3017e-02, -5.0656e-02],
        [-3.7436e-01,  1.5832e-01,  6.8350e-02, -3.1079e-02,  1.3012e-01,
          6.8726e-02, -3.4179e-02, -1.1205e-01, -1.1838e-01,  3.4718e-01,
         -2.0677e-01, -2.9876e-01],
        [-2.7889e-01,  6.6943e-02, -2.3284e-01,  2.0772e-01, -1.9392e-01,
          2.1582e-01, -5.5087e-02,  1.5339e-01, -6.3504e-02, -1.0453e-02,
         -2.8071e-01,  1.6879e-01],
        [-1.7269e-02, -1.6418e-01,  1.7032e-01, -1.4402e-01,  2.9182e-02,
          2.9639e-01,  4.1600e-02, -1.7972e-01,  2.8264e-01, -1.8656e-01,
         -2.1326e-01,  6.6384e-02],
        [ 3.3995e-01, -4.2058e-02,  4.2617e-02,  3.0176e-01,  2.8703e-01,
          2.0077e-01, -5.3796e-02, -2.4586e-01,  5.1566e-02,  1.8216e-01,
          1.8207e-01, -3.8346e-02],
        [ 1.6443e-01, -1.9234e-01, -1.5726e-01, -2.0126e-01, -1.9778e-01,
          8.0897e-02, -2.8629e-02,  7.2212e-02, -1.1353e-01, -2.5219e-01,
          2.5212e-01,  2.1722e-01],
        [ 1.6740e-01, -2.0253e-01, -7.5516e-02, -2.7444e-01,  2.3911e-01,
         -2.7450e-01, -2.8291e-01, -2.8847e-01,  1.7103e-01,  1.9438e-01,
         -4.4032e-03,  9.7326e-03],
        [-1.3268e-01, -1.7842e-01, -3.0912e-01,  1.5077e-01, -1.3325e-01,
         -1.0864e-01,  2.3997e-02, -5.6846e-02, -1.7071e-01, -6.4414e-03,
         -2.1325e-01, -2.1074e-01],
        [-1.0328e-01, -2.4552e-01, -1.1538e-01,  1.6967e-02, -2.7212e-02,
          8.6552e-03,  2.0887e-01,  1.5222e-01, -7.3533e-02, -2.6590e-01,
          6.4525e-02, -1.3361e-01],
        [-1.9377e-01,  1.6943e-02, -2.8712e-02,  3.5309e-01, -2.3133e-01,
          1.7720e-02, -1.6782e-01,  1.1216e-01, -3.3391e-02,  8.4138e-02,
         -2.6752e-01, -2.5023e-01],
        [-9.0235e-02,  5.2082e-02,  1.1604e-01,  7.2105e-03,  1.6985e-01,
         -2.5543e-01,  7.5246e-04,  1.6754e-01, -2.9398e-01, -8.1148e-02,
         -4.8559e-02, -2.4611e-01],
        [-7.8673e-02, -3.3758e-01, -3.9294e-04,  2.7282e-01, -1.8853e-01,
         -5.9206e-03,  7.4341e-02, -7.8850e-02, -8.9645e-03,  2.0286e-02,
          7.1311e-03,  2.3563e-02],
        [-9.1992e-02,  2.0711e-01, -6.4700e-02, -9.0765e-02, -2.1741e-01,
          2.6720e-01,  2.0436e-01, -2.2074e-01,  1.9142e-01, -7.1285e-03,
          1.8319e-01,  1.7909e-02],
        [ 1.5841e-01, -2.3716e-01, -5.6519e-02, -3.8792e-01,  1.7641e-01,
         -4.0762e-03, -1.9137e-01,  3.5279e-01, -7.1387e-02,  1.5702e-02,
         -7.5439e-02,  8.2502e-02],
        [-1.8056e-01, -8.8663e-02, -1.9419e-01,  1.4016e-01, -2.0391e-01,
         -3.6453e-02, -1.1655e-01, -1.6594e-01,  2.9228e-01, -2.5918e-04,
          7.3999e-02,  6.4653e-02],
        [ 3.4276e-02, -2.0629e-01, -4.6682e-02, -1.0486e-01, -1.4785e-02,
         -4.2736e-01, -1.2686e-01,  5.0259e-02, -1.2992e-01, -2.6674e-01,
          1.3625e-01, -5.7999e-02],
        [-1.3376e-01,  4.0792e-01, -4.3560e-02, -3.9301e-01,  8.9436e-02,
          2.5456e-01,  3.6104e-02,  2.1095e-01,  1.3760e-01,  3.2199e-01,
          2.0099e-02,  2.8367e-02],
        [ 1.4789e-01,  3.4478e-02, -1.3230e-01,  2.1536e-01, -3.1367e-02,
         -4.4410e-01, -2.9586e-01, -7.7456e-02,  2.6577e-01,  3.4602e-02,
          2.6733e-01,  2.8829e-01],
        [ 3.8824e-01, -3.5293e-01,  2.4145e-01,  2.8005e-01,  1.4115e-01,
         -8.7599e-02,  4.2833e-01,  1.7706e-02, -1.2627e-01,  8.8407e-02,
         -2.4001e-01, -2.7347e-01]], requires_grad=True)
predict_state.0.bias Parameter containing:
tensor([ 0.2908,  0.3339,  0.1112,  0.3456, -0.0735,  0.3086, -0.0543,  0.1882,
         0.2779,  0.2279,  0.0185, -0.3007, -0.1252, -0.0332, -0.2370, -0.0014,
        -0.1994,  0.1792, -0.2650, -0.1651,  0.2251, -0.2151,  0.0539,  0.0698,
        -0.1781, -0.1371,  0.1047,  0.2330, -0.2018, -0.2381, -0.0583,  0.1818,
         0.2084,  0.1110,  0.1851,  0.1941,  0.3309,  0.1092,  0.1858,  0.2496,
        -0.2673, -0.0383, -0.2415,  0.1111,  0.2407, -0.3690, -0.0516,  0.0262,
        -0.0707,  0.3789,  0.0978,  0.3246, -0.1284,  0.0712,  0.1021, -0.1380,
         0.1388, -0.2538, -0.0751, -0.1299, -0.0224, -0.2689,  0.3625,  0.1045],
       requires_grad=True)
predict_state.2.weight Parameter containing:
tensor([[-0.1166, -0.0567,  0.0823,  ...,  0.1141,  0.1325,  0.0058],
        [-0.0739,  0.0102,  0.0443,  ...,  0.0949,  0.0057,  0.2001],
        [ 0.1014, -0.1027,  0.0943,  ..., -0.0398,  0.0642,  0.0504],
        ...,
        [ 0.1349,  0.5316,  0.0588,  ..., -0.3402,  0.1524,  0.0876],
        [ 0.0341,  0.0195, -0.0137,  ..., -0.0339, -0.0357,  0.0401],
        [-0.0988, -0.0291, -0.1804,  ..., -0.0640,  0.0581, -0.1068]],
       requires_grad=True)
predict_state.2.bias Parameter containing:
tensor([-9.7786e-03,  1.1603e-01,  8.8831e-02,  2.8107e-01,  9.5001e-03,
         4.7490e-02, -2.6357e-02,  8.9261e-02, -1.0153e-01,  3.2277e-02,
        -5.2509e-04, -1.1115e-01, -1.3851e-01, -1.1585e-01, -3.2471e-03,
        -1.0566e-01,  1.1330e-01, -1.1987e-02,  2.4680e-02, -1.7600e-01,
         2.0841e-03, -1.0553e-01, -1.2377e-01, -1.0380e-01, -1.0983e-01,
        -2.5515e-02,  1.2623e-01, -5.9273e-02,  7.7763e-02,  1.9827e-02,
        -1.2586e-01, -1.1432e-01,  2.1448e-02,  6.9549e-02, -3.6994e-02,
        -3.8898e-02,  1.1288e-02,  5.8760e-02, -1.1549e-01, -1.7390e-01,
        -5.7734e-02, -2.1394e-02,  3.9715e-02,  5.5776e-02,  1.1726e-02,
        -1.2573e-02,  8.6645e-03, -5.4080e-02,  6.0752e-02,  6.0604e-02,
        -4.1427e-05, -1.3416e-01, -7.9244e-02, -4.8941e-02, -1.2716e-01,
         1.0636e-01,  1.1302e-01,  6.5573e-02,  4.7611e-02, -1.2837e-01,
        -1.1655e-01,  2.3242e-01,  7.9397e-02, -2.3770e-02],
       requires_grad=True)
predict_state.4.weight Parameter containing:
tensor([[ 0.0418, -0.2215,  0.0221,  0.0437, -0.1970, -0.1698,  0.1427,  0.1991,
         -0.0204, -0.0010,  0.0040,  0.0138,  0.1034, -0.0935,  0.0332,  0.1687,
          0.1247,  0.1149,  0.2368,  0.0261, -0.1982,  0.1204, -0.0128, -0.2227,
         -0.1404, -0.0335, -0.0849, -0.0535, -0.1094, -0.1244, -0.1364, -0.1657,
         -0.2652,  0.1036,  0.1199, -0.0775, -0.0146,  0.1878, -0.2512, -0.0483,
          0.0746,  0.0349, -0.0829,  0.0328, -0.1117,  0.0690, -0.1276, -0.0153,
         -0.0075,  0.1454, -0.1195, -0.1543, -0.1171, -0.0919,  0.0737,  0.1514,
         -0.2109,  0.2117,  0.1660, -0.0729, -0.1882,  0.2095, -0.0496,  0.1096],
        [ 0.1616,  0.2130, -0.1513,  0.0309,  0.1866, -0.0367, -0.1530, -0.2201,
         -0.0769,  0.1769, -0.0474, -0.1247, -0.0535,  0.1270,  0.1714, -0.0205,
         -0.0168, -0.0037, -0.1104, -0.2633,  0.0423,  0.0588,  0.1572,  0.1530,
          0.1936,  0.1596, -0.0509,  0.2121, -0.0938,  0.1924,  0.1235, -0.1189,
         -0.0332, -0.0914, -0.0953, -0.1329, -0.1629,  0.0063,  0.1078, -0.0273,
         -0.1490, -0.0877,  0.2402,  0.1425, -0.0262, -0.2801, -0.1145,  0.1260,
         -0.0442, -0.2611,  0.0388,  0.2505,  0.1588,  0.0608, -0.1551, -0.0078,
          0.0922, -0.2235,  0.0635,  0.1665,  0.0876, -0.1113,  0.1585, -0.0301],
        [ 0.0086,  0.1542,  0.0325, -0.1842, -0.0570,  0.1372,  0.0537, -0.1572,
         -0.0825,  0.0342, -0.0679, -0.1753, -0.2615,  0.0856, -0.2199,  0.0271,
         -0.0744,  0.1374,  0.0477,  0.2255,  0.0195, -0.0877, -0.0427,  0.0243,
         -0.0760, -0.0804,  0.1415, -0.0158,  0.0901, -0.0309,  0.0233, -0.0775,
         -0.0026,  0.1845,  0.1220,  0.1269,  0.1365, -0.1053,  0.1471, -0.1572,
         -0.0556,  0.1898, -0.1877, -0.1912, -0.1098,  0.1749, -0.0904, -0.2915,
         -0.0055, -0.0498, -0.1959, -0.1507,  0.0640,  0.1268, -0.0763,  0.1945,
          0.0132, -0.1299,  0.0818,  0.0091, -0.0139, -0.2074, -0.0523, -0.1358],
        [-0.0295,  0.0395, -0.0562, -0.2190,  0.1564,  0.1363, -0.0010,  0.1311,
          0.1125, -0.0152, -0.1442, -0.0703, -0.1082, -0.1259, -0.1777, -0.0344,
          0.1351,  0.0560,  0.0639,  0.0455,  0.0786, -0.0726, -0.0629,  0.1109,
         -0.1348, -0.1022, -0.0209, -0.0058,  0.1113, -0.1338,  0.1413,  0.0210,
          0.1811, -0.0153, -0.1027,  0.0271,  0.0295,  0.0694,  0.1379, -0.1010,
          0.0642,  0.0345, -0.0654, -0.0008, -0.0123,  0.0380,  0.1170, -0.1119,
         -0.1378,  0.1428, -0.1203, -0.0070, -0.0566,  0.0109, -0.0875,  0.1970,
         -0.0905, -0.0700,  0.1189, -0.2064, -0.0710, -0.1688, -0.1104, -0.1478],
        [-0.0248,  0.1258,  0.1714, -0.1831, -0.0984,  0.1780,  0.0227, -0.0126,
         -0.0825,  0.0602, -0.2170, -0.1505, -0.3097,  0.1602, -0.1161, -0.0244,
         -0.0560,  0.1214, -0.0444,  0.0936,  0.2357, -0.1068, -0.0895, -0.0961,
          0.0062, -0.1791,  0.2823, -0.0606,  0.1545, -0.1317, -0.0352, -0.0717,
          0.0680,  0.1211,  0.2108,  0.0539, -0.0389, -0.1337,  0.1005, -0.2568,
          0.0828,  0.1224, -0.0163, -0.3486, -0.2481,  0.0802, -0.0931, -0.2019,
         -0.1163, -0.0265, -0.0890, -0.0592,  0.1556, -0.0583, -0.0615, -0.0499,
          0.0673, -0.0387, -0.0610, -0.1613,  0.0918, -0.1514, -0.1434, -0.0035],
        [-0.1765, -0.0376, -0.0153, -0.2079,  0.0611,  0.0255,  0.2200, -0.0161,
         -0.1301, -0.0597, -0.0723, -0.1824, -0.1471, -0.1361, -0.2064,  0.1724,
         -0.0142, -0.0223, -0.0582,  0.2047,  0.0717,  0.0120, -0.2435, -0.2241,
          0.0451, -0.1242,  0.1514, -0.0466, -0.0493, -0.2580, -0.1683, -0.0582,
          0.0261,  0.0027,  0.1720,  0.0920,  0.2098,  0.0979, -0.1528, -0.0807,
          0.1400,  0.0970, -0.2853, -0.2738, -0.1000,  0.2656,  0.0878, -0.1784,
         -0.1924,  0.1570, -0.2341, -0.1308, -0.1116,  0.0742,  0.0595,  0.0608,
         -0.0750,  0.1440,  0.0913, -0.0771, -0.1432,  0.0060, -0.2779,  0.0590],
        [ 0.0674, -0.1667,  0.0873,  0.1071, -0.1636, -0.0465,  0.1621,  0.0752,
          0.1727, -0.1519, -0.0622, -0.0393,  0.1030, -0.1350, -0.0643,  0.0929,
          0.1285,  0.1943,  0.1396,  0.1038, -0.0390,  0.1228,  0.0940, -0.1894,
         -0.1210,  0.0683, -0.0102, -0.1436,  0.0723, -0.1667, -0.1756, -0.2337,
         -0.0865,  0.1067,  0.0247, -0.1054,  0.1989,  0.0926, -0.1435,  0.1285,
          0.2042,  0.0785, -0.0758,  0.1255, -0.0518,  0.1090, -0.1186, -0.0182,
         -0.0400,  0.1852, -0.0810, -0.0831, -0.1339, -0.1495,  0.2351,  0.1400,
         -0.1346,  0.2295,  0.1737, -0.0431, -0.1431,  0.3426, -0.1372,  0.0196],
        [-0.0007,  0.1785,  0.1394, -0.2773,  0.1024,  0.0717, -0.0616, -0.0229,
         -0.0133,  0.0927, -0.2823, -0.0774, -0.1759, -0.0960, -0.1966,  0.0885,
         -0.0505,  0.0699,  0.1564,  0.0533,  0.1377,  0.0178, -0.1689, -0.0352,
         -0.0058, -0.1454,  0.1983, -0.0417,  0.1094, -0.2910,  0.1620, -0.2333,
          0.0509, -0.1073, -0.0679,  0.1275, -0.1226, -0.1092,  0.0787, -0.1554,
         -0.0464,  0.1925, -0.0964, -0.3461, -0.2663,  0.2004,  0.0696, -0.0672,
          0.0390,  0.0720, -0.0477, -0.1015,  0.0832,  0.0072,  0.0620,  0.0596,
          0.0013,  0.1526, -0.0801, -0.1527,  0.1069, -0.1076, -0.0940, -0.2770]],
       requires_grad=True)
predict_state.4.bias Parameter containing:
tensor([-0.0111, -0.0463,  0.0964,  0.1592, -0.0268,  0.1041,  0.0312,  0.1532],
       requires_grad=True)
predict_reward.0.weight Parameter containing:
tensor([[-0.0077, -0.1243, -0.3620,  0.2224, -0.2524,  0.0156,  0.2128,  0.0324,
         -0.0696, -0.1894, -0.0341, -0.3460],
        [ 0.1691, -0.2434,  0.0200, -0.0108, -0.0355,  0.3465,  0.2428, -0.0681,
          0.2395,  0.0972, -0.2999,  0.2712],
        [-0.1964, -0.1179, -0.0530,  0.4048,  0.0121,  0.1887, -0.1320,  0.3987,
         -0.0131, -0.2546, -0.2083,  0.1284],
        [ 0.1946, -0.0321, -0.1918, -0.0761, -0.2125,  0.2251, -0.0647,  0.1122,
          0.0036,  0.2804, -0.1762,  0.3584],
        [-0.1648, -0.3215,  0.1152,  0.4062,  0.1331,  0.1641, -0.1033, -0.1190,
         -0.0866, -0.2607, -0.0178, -0.3353],
        [-0.3074,  0.0982,  0.3347,  0.1487, -0.0929, -0.0562, -0.4266, -0.0884,
          0.2651,  0.1354, -0.4147,  0.3048],
        [-0.0793, -0.2548,  0.1215,  0.2645, -0.3279,  0.2277, -0.2188,  0.1089,
          0.4283,  0.0139, -0.3819,  0.3673],
        [-0.0167,  0.0054, -0.0197, -0.1816,  0.0843, -0.1976,  0.1404,  0.1850,
         -0.0968,  0.0404, -0.0091, -0.4864],
        [-0.2504, -0.1180,  0.2204,  0.0558, -0.1875,  0.0395,  0.2127, -0.2136,
          0.0265,  0.1568,  0.1820, -0.0622],
        [ 0.1262, -0.2648, -0.2622,  0.0788, -0.3532,  0.1164,  0.1854, -0.2869,
         -0.0775, -0.0386,  0.0854, -0.1017],
        [ 0.3142,  0.3490,  0.2572,  0.0741,  0.1687, -0.2170, -0.2527, -0.1279,
          0.0767,  0.1372, -0.4119,  0.0087],
        [-0.0008,  0.0017,  0.0984,  0.2883,  0.2915, -0.4051, -0.0579,  0.0725,
         -0.1738, -0.0284, -0.1037, -0.1084],
        [-0.2230,  0.2559,  0.0691,  0.0822,  0.0309,  0.2533,  0.0792, -0.2281,
          0.2554,  0.2721,  0.0453,  0.2103],
        [-0.1720, -0.1107,  0.2775,  0.1659, -0.3562, -0.1396,  0.2014, -0.1493,
          0.4248,  0.2388, -0.3094, -0.0605],
        [-0.2142, -0.0259, -0.1930,  0.2718,  0.0996,  0.3251,  0.2307, -0.3291,
          0.3061,  0.0428, -0.4049, -0.0366],
        [ 0.0986, -0.2457,  0.1432,  0.1520, -0.1055,  0.0646,  0.0699, -0.0725,
          0.1169, -0.3413,  0.4378, -0.2584],
        [ 0.2973, -0.0616, -0.1487, -0.3778, -0.3195, -0.0578, -0.0678, -0.3812,
         -0.0023,  0.1460, -0.0054,  0.2219],
        [ 0.1612,  0.0543, -0.1537, -0.2159,  0.0437,  0.1548, -0.3250,  0.0815,
          0.1142, -0.1749, -0.1474, -0.0942],
        [-0.0190,  0.2904, -0.2028, -0.2447, -0.1180,  0.1942,  0.0793,  0.3585,
         -0.3957, -0.0870,  0.2141, -0.2688],
        [-0.1978, -0.0573,  0.2860, -0.1883, -0.0410, -0.1784, -0.3502, -0.1724,
         -0.1420,  0.2377, -0.4099, -0.0027],
        [ 0.0211, -0.0330,  0.1025,  0.2751,  0.3333, -0.3485,  0.2019,  0.1353,
         -0.3733,  0.0670, -0.1366, -0.4180],
        [ 0.2889,  0.1121, -0.1117, -0.2717,  0.1558,  0.2537,  0.0204, -0.1037,
          0.3567,  0.0737, -0.3791, -0.1022],
        [ 0.1022, -0.2421, -0.0519,  0.4227,  0.0938,  0.1228, -0.2157,  0.0232,
         -0.0393, -0.1164, -0.0108,  0.0673],
        [-0.1521,  0.1901, -0.1192, -0.0251,  0.0529, -0.1524, -0.2244,  0.1038,
         -0.3676, -0.2350,  0.3316, -0.2160],
        [-0.2735, -0.0074, -0.2247,  0.5073,  0.0897, -0.0254, -0.0388, -0.0354,
         -0.0125, -0.0829,  0.1444,  0.0996],
        [ 0.2056,  0.0459, -0.0333, -0.2851,  0.0561,  0.0272, -0.0635,  0.0098,
          0.2898, -0.1065,  0.0341,  0.2294],
        [-0.2566, -0.0776, -0.0192,  0.2769,  0.0497, -0.1625,  0.1188,  0.0568,
          0.0390, -0.3878,  0.2630, -0.3523],
        [-0.0085,  0.0310,  0.0132, -0.0548, -0.0484,  0.3242, -0.1516, -0.1834,
         -0.1257,  0.2720,  0.0239, -0.2605],
        [ 0.1762,  0.0094,  0.0476, -0.2576, -0.1422,  0.3548, -0.2789, -0.2440,
          0.3245,  0.2112, -0.2629,  0.2884],
        [ 0.2223,  0.3115,  0.1360, -0.3026,  0.0630, -0.1585,  0.0833, -0.3728,
         -0.0316,  0.1618, -0.2533,  0.0765],
        [ 0.2152,  0.2909,  0.1304,  0.0933,  0.1196, -0.2639, -0.3179,  0.0761,
         -0.0947, -0.3253, -0.0490,  0.2325],
        [-0.1390,  0.1897,  0.0220, -0.1478,  0.3338,  0.1358,  0.3090,  0.0902,
          0.0958, -0.2315,  0.0140, -0.4471],
        [ 0.2612, -0.2356, -0.0969,  0.1897, -0.2303, -0.1090, -0.2026, -0.2669,
         -0.0294,  0.0671, -0.1486,  0.2727],
        [ 0.2153,  0.1861, -0.1341, -0.3831,  0.0159,  0.1519, -0.1979,  0.1097,
         -0.2774, -0.0623,  0.1965, -0.2321],
        [-0.1551,  0.2313, -0.0429,  0.1895, -0.1484,  0.0668, -0.0097,  0.3575,
         -0.0721,  0.0421,  0.3523, -0.4104],
        [-0.0943, -0.2689, -0.0496,  0.4145,  0.0966,  0.1663, -0.1847, -0.0984,
         -0.3298, -0.0362,  0.0797, -0.2808],
        [ 0.3251,  0.3639,  0.2497,  0.0197, -0.0077,  0.0443, -0.1298, -0.0516,
         -0.0172,  0.0656, -0.0210,  0.2408],
        [ 0.3324, -0.0190, -0.1417, -0.3118, -0.2295, -0.0610,  0.2755, -0.2162,
          0.1290, -0.1813, -0.2249,  0.0756],
        [-0.3688, -0.2495,  0.2459,  0.2304, -0.0631,  0.1172,  0.2952,  0.0101,
         -0.0371, -0.3943,  0.1823, -0.0908],
        [ 0.0799,  0.0533, -0.1766, -0.1980,  0.3384,  0.0147, -0.0874, -0.0389,
         -0.4621, -0.1622,  0.3357, -0.3293],
        [ 0.1400,  0.0384,  0.0248, -0.2888,  0.1854, -0.2888,  0.0617, -0.1224,
         -0.0551,  0.1158,  0.0616, -0.2554],
        [-0.0497, -0.2829, -0.1595,  0.0291, -0.0068, -0.0208, -0.2274,  0.0376,
          0.2625, -0.1248, -0.2810,  0.3560],
        [-0.3592,  0.0084,  0.0926,  0.3227, -0.1044, -0.0814,  0.2530, -0.0589,
         -0.4243,  0.1345,  0.1539, -0.3821],
        [-0.2310, -0.2864,  0.0223,  0.4734, -0.3063,  0.0636,  0.1824,  0.0854,
         -0.0320,  0.1813, -0.2309,  0.0460],
        [-0.1170,  0.0869, -0.1775,  0.2038, -0.0349, -0.3065,  0.0480,  0.3145,
          0.0030, -0.1392,  0.0288, -0.4408],
        [-0.0349,  0.2907, -0.1992, -0.2578,  0.0249, -0.2438,  0.0552,  0.3116,
          0.0339, -0.0587,  0.2250,  0.2729],
        [-0.2679,  0.0804,  0.1827,  0.2239, -0.2118,  0.2283, -0.0194,  0.2030,
         -0.3832,  0.0426, -0.0921, -0.3773],
        [-0.1102, -0.0538,  0.1880,  0.3449, -0.0667, -0.3018, -0.0517,  0.2655,
         -0.2497,  0.0373, -0.0737, -0.2477],
        [ 0.1099,  0.2903,  0.1276, -0.2586, -0.1872, -0.1585, -0.2995, -0.2088,
          0.2311,  0.3931, -0.0777, -0.0621],
        [-0.0067, -0.0715,  0.1444, -0.2228, -0.2047,  0.1017,  0.0483,  0.1055,
          0.2553,  0.1649,  0.1078,  0.3610],
        [ 0.3353, -0.2381, -0.2151, -0.1967, -0.3023, -0.0727,  0.0521,  0.1982,
          0.2572,  0.2810, -0.3532,  0.2886],
        [-0.1862,  0.2616, -0.2069, -0.4491,  0.0447,  0.2305,  0.1228,  0.0174,
          0.1057, -0.1249,  0.1331,  0.2523],
        [-0.1694,  0.1509,  0.3106, -0.0872, -0.2535, -0.0147, -0.2259, -0.3481,
          0.3674,  0.1496, -0.3913,  0.0803],
        [ 0.1196,  0.2882,  0.0804, -0.0384,  0.1582, -0.1221,  0.0444,  0.2975,
         -0.1675, -0.1014,  0.0479, -0.3783],
        [ 0.0258,  0.1540,  0.0527,  0.2423,  0.2120, -0.2136,  0.1581,  0.4084,
         -0.0053, -0.1702, -0.0507, -0.1556],
        [-0.0173, -0.1732,  0.2467, -0.0739, -0.0544, -0.1656,  0.2258, -0.0613,
          0.2276,  0.1787, -0.1389, -0.0165],
        [-0.2159,  0.0346, -0.0325, -0.2850,  0.1746, -0.3031, -0.1147,  0.4263,
          0.0780, -0.2614,  0.1288,  0.1187],
        [ 0.1411,  0.2520,  0.0547, -0.1065,  0.2295, -0.1625, -0.0953,  0.3224,
          0.0088, -0.1909, -0.0286,  0.1594],
        [ 0.1169,  0.0561,  0.1192, -0.1095,  0.1353,  0.1405,  0.1507, -0.1335,
         -0.2367,  0.1789,  0.2575,  0.1472],
        [-0.0757,  0.0588,  0.2309, -0.3288,  0.1074, -0.1424, -0.0747, -0.3656,
          0.2309,  0.0186, -0.1685,  0.1963],
        [ 0.1003,  0.0243, -0.0642,  0.1192, -0.0211, -0.2184, -0.1041, -0.0943,
         -0.1685,  0.1505,  0.2184, -0.0769],
        [-0.1259, -0.0909,  0.0307,  0.0644,  0.1500, -0.1482,  0.0247, -0.1252,
         -0.1751, -0.1474,  0.1435, -0.2869],
        [ 0.2458, -0.0792, -0.1238, -0.0587, -0.0054,  0.3179, -0.3211, -0.3797,
         -0.0780,  0.0439, -0.0625,  0.4559],
        [-0.0366, -0.1435, -0.2053,  0.3384, -0.3782, -0.0103,  0.0882, -0.0515,
         -0.0174,  0.1993, -0.1555,  0.4781]], requires_grad=True)
predict_reward.0.bias Parameter containing:
tensor([ 0.0083,  0.2521, -0.0252,  0.2929, -0.3074, -0.1939, -0.1491, -0.2155,
        -0.2462,  0.1590,  0.2607, -0.1581, -0.1327,  0.2374,  0.3141, -0.2477,
         0.1721, -0.1065, -0.2908,  0.1105, -0.1842,  0.1907, -0.1241, -0.3015,
        -0.3019,  0.1242, -0.2955, -0.0871, -0.1162,  0.2517, -0.1860,  0.0208,
         0.1646,  0.0936, -0.1099, -0.3217,  0.0961,  0.1850, -0.2476, -0.2833,
         0.0405,  0.2329, -0.1722,  0.0572, -0.1739, -0.0796, -0.0225, -0.2494,
         0.0101,  0.0154,  0.2873,  0.2304,  0.2651, -0.0850, -0.1553,  0.0449,
        -0.2332,  0.0214, -0.0585,  0.2447, -0.0795, -0.3204,  0.2367,  0.1838],
       requires_grad=True)
predict_reward.2.weight Parameter containing:
tensor([[ 0.0884, -0.0232,  0.0270,  ...,  0.0434, -0.0400,  0.1686],
        [-0.0314, -0.2032,  0.0406,  ...,  0.1632, -0.1005, -0.0876],
        [-0.0503,  0.0017,  0.0011,  ..., -0.0323, -0.0734,  0.0047],
        ...,
        [-0.0204,  0.1984,  0.0414,  ..., -0.1746,  0.0750,  0.0428],
        [-0.0047, -0.2134, -0.0665,  ...,  0.1580, -0.0446, -0.2222],
        [-0.1009,  0.0632,  0.0014,  ..., -0.0918,  0.1801,  0.1941]],
       requires_grad=True)
predict_reward.2.bias Parameter containing:
tensor([-0.0413, -0.1726, -0.1279,  0.1184, -0.0304,  0.1802,  0.0939,  0.0912,
        -0.0896, -0.0442,  0.0336, -0.0704,  0.0586, -0.1287,  0.0412, -0.1000,
        -0.1316,  0.0034,  0.0927,  0.0376,  0.0071, -0.0589,  0.0053,  0.0355,
         0.0056,  0.0313, -0.1322, -0.0088, -0.0597,  0.1466,  0.1137, -0.0271,
        -0.0100,  0.0812, -0.0102, -0.0555, -0.1128,  0.0761, -0.0894, -0.0385,
        -0.0489, -0.0005, -0.0609, -0.1061, -0.0487, -0.1303,  0.1070,  0.0700,
         0.0552,  0.0518,  0.1163, -0.0074, -0.0858,  0.1588, -0.0061, -0.0004,
         0.0994,  0.1234, -0.0771,  0.0662, -0.0426,  0.0303,  0.0033, -0.0041],
       requires_grad=True)
predict_reward.4.weight Parameter containing:
tensor([[-0.2437,  0.1352,  0.2257, -0.2371, -0.2412, -0.1282, -0.1000, -0.1461,
          0.2485,  0.1516, -0.1007,  0.0934,  0.1658,  0.2386,  0.2789,  0.1683,
          0.1638,  0.1496, -0.1943, -0.0564,  0.1906,  0.2254, -0.2172, -0.1634,
         -0.1361, -0.1370,  0.2927,  0.1524, -0.1129, -0.2313, -0.0753, -0.1051,
          0.1566, -0.2331, -0.1482, -0.0570,  0.1476, -0.1439,  0.0752, -0.0572,
         -0.0924, -0.1509, -0.0635,  0.1054, -0.3126,  0.1590, -0.1019, -0.0353,
         -0.2159,  0.2006, -0.1127, -0.2034,  0.1379, -0.0189,  0.1046,  0.2552,
         -0.1338, -0.0048,  0.1345, -0.0121,  0.1981, -0.1927,  0.1113, -0.2421]],
       requires_grad=True)
predict_reward.4.bias Parameter containing:
tensor([-0.1345], requires_grad=True)
mean: -101.4463630980607 std 95.23940330301296
